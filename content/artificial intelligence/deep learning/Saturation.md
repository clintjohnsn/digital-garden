When the output of an artificial neuron is in the ’flat’ part
e.g where σ ′ (z) ≈ 0 for [[Sigmoid or Logistic Activation]]

When the neural networks are saturated, gradient steps may not make much progress (as the gradient is very small), even though the loss is large